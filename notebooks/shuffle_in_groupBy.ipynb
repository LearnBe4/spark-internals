{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8213187-a564-42d7-8645-8f5f42d66356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire_Department_Calls_For_Service__2016__20240816.csv\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/spark/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f692c9-35ec-49c0-8059-931c2277355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/30 15:30:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "             .builder\n",
    "             #.enableHiveSupport()\n",
    "             .master(\"spark://spark-master:7077\")\n",
    "             .config(\"spark.sql.warehouse.dir\", \"/opt/spark/spark-warehouse\")\n",
    "             .getOrCreate()\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2cf862-165e-4886-8738-36489fcdde31",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df=(\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", False)\n",
    "    .load(\"/opt/spark/data/Fire_Department_Calls_For_Service__2016__20240816.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca348c4-7fb8-4602-ac8f-64f6e243c6d1",
   "metadata": {},
   "source": [
    "### Reading CSV without explict schema will trigger a job, it read 1st record of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc97049-e651-47fe-bc16-eaf17621edfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52246d-3ee8-4707-9564-3aaea7f290a4",
   "metadata": {},
   "source": [
    "### To avoid reading of files for each action, let's cache it\n",
    "### Count will trigger 3 jobs, \n",
    "1. For File scan \n",
    "2. File scan and cache\n",
    "3. Read from cache and find count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a26f32-48a9-47b9-9833-5a7949fbb283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 15:31:09 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2748979"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.cache()\n",
    "raw_df.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d22a6cd5-e7dc-4b38-aabb-8f2a3d9c89f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns names:\n",
      "['Call Number', 'Unit ID', 'Incident Number', 'Call Date', 'Watch Date', 'Received DtTm', 'Entry DtTm', 'Dispatch DtTm', 'Response DtTm', 'On Scene DtTm', 'Transport DtTm', 'Hospital DtTm', 'Call Final Disposition', 'Available DtTm', 'Address', 'City', 'Zipcode of Incident', 'Battalion', 'Station Area', 'Box', 'Original Priority', 'Priority', 'Final Priority', 'ALS Unit', 'Call Type Group', 'Number of Alarms', 'Unit Type', 'Unit sequence in call dispatch', 'Fire Prevention District', 'Supervisor District', 'Neighborhooods - Analysis Boundaries', 'RowID']\n",
      "Formatted columns names:\n",
      "['call_number', 'unit_id', 'incident_number', 'call_date', 'watch_date', 'received_dttm', 'entry_dttm', 'dispatch_dttm', 'response_dttm', 'on_scene_dttm', 'transport_dttm', 'hospital_dttm', 'call_final_disposition', 'available_dttm', 'address', 'city', 'zipcode_of_incident', 'battalion', 'station_area', 'box', 'original_priority', 'priority', 'final_priority', 'als_unit', 'call_type_group', 'number_of_alarms', 'unit_type', 'unit_sequence_in_call_dispatch', 'fire_prevention_district', 'supervisor_district', 'neighborhooods_-_analysis_boundaries', 'rowid']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original columns names:\\n{raw_df.columns}\")\n",
    "renamed_df=raw_df\n",
    "for cols in renamed_df.columns:\n",
    "    renamed_df=renamed_df.withColumnRenamed(cols, cols.lower().replace(\" \",\"_\"))\n",
    "\n",
    "print(f\"Formatted columns names:\\n{renamed_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8887d76f-72d8-43d8-9a2b-dd873667ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_types_df=renamed_df.select(\"unit_type\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "258c3ddf-adf2-4647-8e04-8766adcced98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unit_types_df.rdd.getNumPartitions() # --> This will create a new job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf91dfd-408d-4c11-9051-a2e17771244e",
   "metadata": {},
   "source": [
    "### Why .rdd.getNumPartitions() causes a Spark job:\n",
    "- .drop_duplicates() requires a shuffle\n",
    "- To remove duplicates, Spark must group similar records together.\n",
    "- This grouping causes a shuffle, which redistributes data across partitions.\n",
    "- Shuffles create new partitions.\n",
    "- After shuffling, Spark reorganizes the data into new partitions.\n",
    "- Accessing .rdd is lazy, Simply calling .rdd on a DataFrame does not trigger computation. It just converts the logical plan into an RDD lineage.\n",
    "- Calling .rdd.getNumPartitions() forces execution\n",
    "- Spark needs to materialize the RDD to know the actual number of partitions.\n",
    "- To do this, Spark finalizes the logical plan, optimizes it, and builds a physical execution plan.\n",
    "- This process triggers a Spark job\n",
    "- Since Spark must execute the shuffle and physically create partitions in memory, a Spark job is triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8db95768-3874-49ff-9007-e3e22195bfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "renamed_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97f13b8b-c70f-4daf-a291-fc278141fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "agg_df=renamed_df.groupBy(\"unit_type\").agg(count(\"*\").alias(\"cnt_of_unit_type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b4fec83-2c96-4ed5-bb46-249fa729e816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:====================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da3ad82a-26aa-4c9f-a8fd-c3cca3dacd46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "171831bd-73f1-4be2-aa4c-fdc6046e8f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.adaptive.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "472e7e0b-aeae-4210-8aeb-6e30372a1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a96ac9be-1310-4fac-a5d6-dbdac7683ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0534eb8e-89d0-4dd5-8e91-8c449adb954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7bf0f95-796e-42e8-a5f5-5918fb4c3fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6430d607-8091-444b-b847-0e77be7d4747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
