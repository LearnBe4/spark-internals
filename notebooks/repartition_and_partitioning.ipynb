{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8918be-6165-4e7a-b3c1-b1a0bbe44aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire_Department_Calls_For_Service__2016__20240816.csv\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/spark/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae48aee-754b-408c-bbe3-8cd3ba1da582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/28 18:47:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "             .builder\n",
    "             #.enableHiveSupport()\n",
    "             .master(\"spark://spark-master:7077\")\n",
    "             .config(\"spark.sql.warehouse.dir\", \"/opt/spark/spark-warehouse\")\n",
    "             .getOrCreate()\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540d3090-d30c-45f4-9b4a-d7cf90576c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1015c994-b479-4e4f-95b9-08083b1feeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd6b96e-f970-4b09-8dce-b59db024aa00",
   "metadata": {},
   "source": [
    "# Constructs of repartititioning in spark:\n",
    "\n",
    "1. `def repartition(numPartitions: Int): Dataset[T]`: \n",
    "- Returns a new Dataset that has exactly numPartitions partitions. \n",
    "- Here data gets equally distributed into numPartitions based on round-robin algorithm. It doesn't use hash partitioning.\n",
    "\n",
    "2. `def repartition(partitionExprs: Column*): Dataset[T]`: \n",
    "- Returns a new Dataset partitioned by the given partitioning expressions, using spark.sql.shuffle.partitions as number of partitions. \n",
    "- The resulting Dataset is hash partitioned.\n",
    "- This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL) where framework ensures that all rows of a key will land in same partition/reducer. \n",
    "- It doesn't ensure that all rows of each key will land in unique partition/reducer. \n",
    "- So rows of more than one unique key can land into same partition/reducer.\n",
    "\n",
    "3. `def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T]`: \n",
    "- Returns a new Dataset partitioned by the given partitioning expressions into numPartitions. \n",
    "- The resulting Dataset is hash partitioned. This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL).\n",
    "- The only difference between this and above one is, here numPartitions is not default value(spark.sql.shuffle.partitions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d10af0a-f266-4b86-b90b-48d2d3e51fc3",
   "metadata": {},
   "source": [
    "Let’s use the below dataset to test different scenarios.\n",
    "About Data: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8cbabd-5c6f-4f07-8311-8e37f7b1f9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_df=(\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", False)\n",
    "    .load(\"/opt/spark/data/Fire_Department_Calls_For_Service__2016__20240816.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5287272-946a-4de2-9b1f-7e5b2b82a003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9fb152-15c6-4825-be1c-3335d19c9fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd54af36-1ae7-46c1-9eef-ebec4ad392d4",
   "metadata": {},
   "source": [
    "Scenarios 1: Writing Data without physical partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef4915-a8cf-4a0f-a8c5-8edb3968b129",
   "metadata": {},
   "source": [
    "Using repartition(numPartitions: Int) without partitionBy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a82350-4c1a-44d6-b724-ecd62a1a0aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    ".repartition(5)\n",
    ".write\n",
    ".mode(\"overwrite\")\n",
    ".save(\"dummy_location_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a62f6-9a2a-45a2-9033-8794991d525f",
   "metadata": {},
   "source": [
    "Spark will use round robin algorithm and distribute the data evenly across 5 output files. There will not be any use of hash partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342eb77-336e-46b9-b8a8-9f1a41198617",
   "metadata": {},
   "source": [
    "2. Using repartition(partitionExprs: Column*) without partitionBy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7c95b-ab83-4977-83d9-f69c2bcc72a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    ".repartition($\"dept\")\n",
    ".write\n",
    ".mode(\"overwrite\")\n",
    ".save(\"dummy_location_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b7bc1-7994-49d8-aede-3b85722c7153",
   "metadata": {},
   "source": [
    "In this case, Spark will try to create 1 to spark.sql.shuffle.partitions(200 by default) files based on hash algorithm. Let’s assume that the number of partition of df is 200(= default value of spark.sql.shuffle.partitions).\n",
    "\n",
    "file_number=hash(KEY) modulus(%) Number of partitions\n",
    "\n",
    "Here, we have 12 distinct values of dept column, and hash value of each string is unique. Since we are taking modulus of hash value of key with number of partition, so we will get at max 12 output files(if remainder of modulus are different for each key). There is high possibility that rows of more than one key can land in the same file resulting to less number of files than 12. This is because remainder(modulus) can be same. for ex:\n",
    "\n",
    "210%200 = 410%200 = 610%200 = 10\n",
    "So, even if the hash values of each string is different, result of modulus can be same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aade4a-baa0-467e-bbf4-41de8fbe0bf9",
   "metadata": {},
   "source": [
    "3. Using repartition(numPartitions: Int, partitionExprs: Column*) without partitionBy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce3cb0-9691-4641-8f5a-9c47db28f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    ".repartition(4, $\"dept\")\n",
    ".write\n",
    ".mode(\"overwrite\")\n",
    ".save(\"dummy_location_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3fc6e-1052-4bb3-889d-4e49edefc274",
   "metadata": {},
   "source": [
    "In this case, it’s same as previous one but here the numPartitions is provided as a parameter of the function instead of using spark.sql.shuffle.partitions.\n",
    "\n",
    "file_number=hash(KEY)%4\n",
    "\n",
    "So, there can be at max we can 4 files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995285c-7bcf-4cfd-b5cd-ed6929f56475",
   "metadata": {},
   "source": [
    "Scenarios 2: Writing Data with physical partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64dcc32-aceb-4d59-8736-c4a6ecb020f8",
   "metadata": {},
   "source": [
    "Using partitionBy without repartition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba6abd-b688-4988-b098-c7a80441db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    ".write\n",
    ".partitionBy(\"dept\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"dummy_location_4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047c3934-6996-43e6-8693-f2b86e7ee1fd",
   "metadata": {},
   "source": [
    "Here, spark will use spark.sql.shuffle.partitions/df.rdd.getNumPartitions to decide number of files in each partition’s directory. Let’s say, rows of dept=xyz is scattered over n number of partitions, spark will write n files for partition dept=xyz in dummy_location_4/dept=xyz directory. Similarly, for any other dept, data is scattered over some random partitions, spark will write the number of files same as number of partitions where the rows are available for the given dept.\n",
    "\n",
    "In this example, since number of partition of df is 200, spark will try to write at max 200 files/directory depending on how many spark partitions contains the rows of given dept.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb08a1b-70a3-4d25-9abb-3d4037fdd893",
   "metadata": {},
   "source": [
    "2. Using partitionBy with repartition(numPartitions):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7711e-fd5e-4d7e-aa68-732d6467d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    ".repartition(2)\n",
    ".write\n",
    ".partitionBy(\"dept\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"dummy_location_5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99b6c67-41f0-4e44-90de-b93171904998",
   "metadata": {},
   "source": [
    "Here, due to repartition(2), spark will push the data from spark.sql.shuffle.partitions/df.rdd.getNumPartitions partitions to only 2 partitions. Before writing the data to storage, spark will reduce the partition to 2. Now for any key, data will be available in any of the 2 partitions or in both partitions. So, here we can get at max 2 files/partition’s directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391fe731-3242-49fe-b877-34fca6fee5fe",
   "metadata": {},
   "source": [
    "3. Using partitionBy with repartition(partitionExprs: Column*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6aadb8-2998-4322-8229-2e7a460b07b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    ".repartition($\"dept\")\n",
    ".write\n",
    ".partitionBy(\"dept\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"dummy_location_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b9a6a-c5de-4a90-8168-c110d49d3d7b",
   "metadata": {},
   "source": [
    "Because of df.repartition($\"dept\") the number of partition will change to spark.sql.shuffle.partitions but data will be available on partition ≤ number of distinct values of dept, remaining partitions will be empty.\n",
    "\n",
    "Because of repartition(dept), all rows of a key will be available in only one of the partition. Reparation ensures that the rows of same key will not land in more than one partitions. But one partition may or may not contain all the rows of more than one key.\n",
    "\n",
    "Since the partitioning column is also same i.e dept, and all rows of a given dept can be found only on 1 spark partition due to repartition($”dept”). So, we will have only 1 file/Partition on file system.\n",
    "\n",
    "Here, we have one more scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa95ee-f061-4b09-ad76-958df27ea003",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    ".repartition($\"gender\")\n",
    ".write\n",
    ".partitionBy(\"dept\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"dummy_location_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194e3d03-0fba-4f6c-b162-0c736a08f6b6",
   "metadata": {},
   "source": [
    "Guess, what will be the number of files written to storage?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e77a2-db54-411f-b290-94457c02e4cd",
   "metadata": {},
   "source": [
    "4. Using partitionBy with repartition(numPartitions: Int, partitionExprs: Column*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608e05f-1d54-4126-b956-9de6b73a2bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    ".repartition(4, $\"dept\")\n",
    ".write\n",
    ".partitionBy(\"dept\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"dummy_location_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce7e73-97cf-483f-bc35-1749d5f5a099",
   "metadata": {},
   "source": [
    "This is same as previous case, but here modulus= hash_value(dept)%4. So all the rows of dept can be at max scattered across 4 partitions. But, all rows of the same key(dept) can be found in any one of the partitions.\n",
    "\n",
    "So, all the rows of each dept can be found in any one of the partitions and since the physical partitions column is also dept, we will have only 1 file/Partition on file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eff248-512e-45b0-9f60-72e246c3c5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a200ff6-c7d0-4e8f-9ad1-6fcea1b71ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
